<!DOCTYPE html>
<!-- saved from url=(0033)https://QicongXie.github.io/end2endvc/ -->
<html lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>End-to-end One-shot Voice Conversion with Perturbation based Speech Disentanglement</title>
<meta name="generator" content="Jekyll v3.9.0">
<meta property="og:title" content="TODO: title">
<meta property="og:locale" content="en_US">
<link rel="canonical" href="https://QicongXie.github.io/end2endvc/">
<meta property="og:url" content="https://QicongXie.github.io/end2endvc/">
<meta name="twitter:card" content="summary">
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="style.css">
  </head>
  <body data-new-gr-c-s-check-loaded="14.1001.0" data-gr-ext-installed="">
    <section class="page-header">
    <!-- <h1 class="project-name">Demo PAGE</h1> -->
    <!-- <h2 class="project-tagline"></h2> -->
      
      
    </section>

<section class="main-content">
      <h1 id=""><center>End-to-end One-shot Voice Conversion with Perturbation based Speech Disentanglement</center></h1>

<center> Qicong Xie, Shan Yang, Yi Lei, Lei Xie, Dan Su  </center>
<center> Northwestern Polytechnical University, Xi'an, China </center>
<center> Tencent AI Lab, China</center>

<h2>0. Contents</h2>
<ol>
  <li><a href="#abstract">Abstract</a></li>
  <!-- <li><a href="#Comparison">Demos -- Comparison with other methods</a></li>
  <li><a href="#prediction">Demos -- The necessity of prosody components</a></li>
  <li><a href="#control">Demos -- Speech synthesis by manual control</a></li>
  <li><a href="#demo">Demos -- multi-speaker multi-style TTS with single-speaker single-style training data secnarios</a></li> -->
</ol>

<br><br>
<h2 id="abstract">1. Abstract<a name="abstract"></a></h2>
<p> 
  Voice conversion aims at converting the timbre of the source speaker to another target while maintaining the content and prosody of the source speech. However, existing approaches still have some shortcomings in the converted speech because of the unsatisfied content disentanglement or the mismatch between the acoustic model and vocoder. In this paper, we propose a fully end-to-end method to conduct high-quality voice conversion, where the information perturbation is adopted to achieve effective timbre and content disentanglement. To further model the paralinguistic information of the source speech, we propose a speaker-dependent prosody module to convert the prosodic trend of the source speaker into the target domain. Besides, we also set up the one-shot voice conversion through a continuous speaker space modeling. Experimental results indicate that the proposed end-to-end method significantly outperforms the state-of-the-art models in terms of quality, naturalness, and speaker similarity.
</p>
	
  <center><table frame=void rules=none>
    <tr><center><img src='raw/fig/overall.png'></center></tr>
    <tr>
    <td><img src='raw/fig/content.png' border=0></td>
    <td><img src='raw/fig/spknet.png' border=0></td>
    <td><img src='raw/fig/pitchnet.png' border=0></td>
    </tr></table>
<br><br>

</table>
      <footer class="site-footer">
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com/">GitHub Pages</a>.</span>
      </footer>
    </section>
</body></html>
